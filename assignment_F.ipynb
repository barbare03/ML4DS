{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3603b9b7",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Rchatru/ML4DS/blob/master/assignment_F.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "  </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hRJcok36aOvj",
      "metadata": {
        "id": "hRJcok36aOvj"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c460622d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Detect if running in Google Colab\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "    print(\"Running in Google Colab. Installing some required libraries...\")\n",
        "    !pip install category_encoders wandb shap tsfresh pyts sktime[all_extras] evalml featuretools imbalanced-learn pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1e181fc3affd50",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-30T13:03:14.186656Z",
          "start_time": "2023-11-30T13:03:13.866334Z"
        },
        "id": "fa1e181fc3affd50"
      },
      "outputs": [],
      "source": [
        "# Basic Libraries & data manipulation\n",
        "import os\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statistics\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Feature Encoding\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
        "import category_encoders as ce # pip install category_encoders\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
        "\n",
        "# Feature Engineering\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Modelling & Evaluation\n",
        "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "\n",
        "\n",
        "## Not important for now. Could be useful at the model training stage\n",
        "# import wandb\n",
        "# import shap\n",
        "## For installing them in Colab do:\n",
        "## pip install wandb\n",
        "## pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ee5f5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1bee4f6",
      "metadata": {},
      "source": [
        "### Import the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce38781",
      "metadata": {},
      "source": [
        "\n",
        "#### Option 1- If working in Google Colab:\n",
        "\n",
        "1. Add the following files to your drive:\n",
        "https://drive.google.com/file/d/1gKJmVCODGcpCfgVtOgUzlxH1Q1Vf0KaR/view?usp=sharing\n",
        "https://drive.google.com/file/d/1nydN6iEc-WEn1ZcOw3K1vjEonAuqWWOh/view?usp=sharing\n",
        "2. In the drive interface, create a shortcut for both files\n",
        "3. Uncomment the cell and mount your drive using the code below\n",
        "\n",
        "*Not necessary, now the script detects if running in Colab and runs the required config steps*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gpczMIXP12yu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpczMIXP12yu",
        "outputId": "ff86bc42-0086-453b-8679-fd781fce4be4"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#df = pd.read_csv('/content/drive/MyDrive/train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42699bc",
      "metadata": {},
      "source": [
        "#### Option 2- Import the data locally from Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hzgj3iK38jDx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzgj3iK38jDx",
        "outputId": "d0726717-2f81-4455-c79a-cba6ebe9dde3"
      },
      "outputs": [],
      "source": [
        "#filename = 'train.csv'\n",
        "#df = pd.read_csv(os.path.join(os.getcwd(),filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ea7514",
      "metadata": {},
      "outputs": [],
      "source": [
        "if in_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "    comp_df = pd.read_csv('/content/drive/MyDrive/competition.csv')\n",
        "else:\n",
        "    filename = 'train.csv'\n",
        "    filenamecomp = 'competition.csv'\n",
        "    df = pd.read_csv(os.path.join(os.getcwd(),filename))\n",
        "    comp_df = pd.read_csv(os.path.join(os.getcwd(),filenamecomp))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea2eb10",
      "metadata": {},
      "source": [
        "## EDA. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c97b43",
      "metadata": {},
      "source": [
        "### Basic Info\n",
        "\n",
        "We have 329975 samples and 6 features. The features are: `Year`, `Month`, `Consumer_type`, `Consumption`, `Consumer_number` and `Installation_zone`.\n",
        "\n",
        "The variables `Year`, `Month` and `Consumption` are numerical (type `int64`), while `Consumer_type`, `Consumer_number` and `Installation_zone` are textual (type `object`).\n",
        "\n",
        "In this case the *target* variable is `Consumer_type`, which is a categorical variable with 7 possible values: `['domestic', 'industrial', 'rural commercial', 'construction', 'low income families', 'rural domestic', 'rural expansion']`. The `domestic`, `industrial` and `low income families` are self-explanatory. The rural* categories are related to agro-production consumers in varying sizes:\n",
        "- rural domestic: small/familiar companies\n",
        "- rural commercial: mid-sized companies\n",
        "- rural expansion: others agro-production\n",
        "\n",
        "*Note that a priori, the agro-production user's water consumption could be very season dependant (because of the crops cycles and the weather), so it could be interesting to explore this in the future.*\n",
        "\n",
        "There are no NaN values in the dataset. But in this case, it is important to note the existence of the consumption value `0` in the feature `Consumption`. This could represent a missing value or actually, a zero water usage for that particular consumer in a specific month.\n",
        "\n",
        "We have 7 years of data, from 2013 to 2020 (except 2015), All the years are complete, with 12 months each.\n",
        "\n",
        "- Years available:  **[2013, 2014, 2016, 2017, 2018, 2019, 2020]**\n",
        "- Months:  **[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]**\n",
        "- Consumer types:  **['domestic', 'industrial', 'rural commercial', 'construction', 'low income families', 'rural domestic', 'rural expansion']**\n",
        "- Number of Installation zones: **49**, values from **Installation_zone 1 to Installation_zone 49**\n",
        "- Number of unique Consumers:  **27632**\n",
        "- Consumption values ranging from **0 to 4978 m3**\n",
        "- Number of water consumption values equal 0 m3: **61658**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68aea46f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.head(), '\\n')\n",
        "print(df['Consumer_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894d79f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset basic info\n",
        "display(df.info())\n",
        "print('Years available: ', df.Year.unique().tolist())\n",
        "print('Months: ', df.Month.unique().tolist())\n",
        "print('Number of months available each year: ', df.groupby('Year')['Month'].nunique().tolist())\n",
        "print('Consumer types: ', df.Consumer_type.unique().tolist())\n",
        "print(f'Number of Installation zones: {df.Installation_zone.nunique()}, values from {df.Installation_zone.unique()[0]} to {df.Installation_zone.unique()[-1]}')\n",
        "print('Number of unique Consumers: ', df.Consumer_number.nunique())\n",
        "print(f'Consumption values ranging from {df.Consumption.min()} to {df.Consumption.max()} m3')\n",
        "print(f'Number of water consumption values equal 0 m3: {df[df.Consumption == 0].shape[0]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bb16b7",
      "metadata": {},
      "source": [
        "### Percentage of 0 m3 Consumption value for each year, month and Consumer type\n",
        "\n",
        "- Analyzing the proportion of zero values for each year, we see small diferences. The values are almost equally distributed between the years. The highest percentage is for the years 2018, 2020 (~ 15 %), and the lowest for the year 2013, 2017, 2016 (~ 13 %). *Note that the percentages are relative to the total number of zero consumption values. Eg. out of the 61658 existing zero values, 15.19 % corresponds to the year 2018. If we calculate the percentages relative to the total number of samples, the values are much lower, from 2.5 to 2.8 %.*\n",
        "\n",
        "- If we repeat the same for each month, we can see more differences. The months with the highest percentage of zero values are January, February and December (10.09, 9.18 and 9.99 % respectively). On the other hand, the lowest percentages are for June, July and August (7.59, 7.22 and 6.06 % respectively). *Is there a season trend?*\n",
        "\n",
        "- Finally, if we analyze by Consumer type, it is considerable the great amount of zero values for the `rural expansion` category (41 %). Also `construction` (35 %) has a high percentage of zero values. On the other end, there are the `domestic` and `low income families` categories (16.30 and 6.60 % respectively). *Note that in this case, the percentages are relative to the total number of samples for each Consumer type. Eg. out of the 890 rural expansion samples, 41,34 % (368) of them corresponds to zero consumption values.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcb8f8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Proportion of 0 m3 Consumption value by year [%]: \\n', df[df.Consumption == 0].value_counts(['Year'], normalize=True)*100, sep='', end='\\n\\n')\n",
        "print('Proportion of 0 m3 Consumption value by month [%]: \\n', df[df.Consumption == 0].value_counts(['Month'], normalize=True)*100, sep='', end='\\n\\n')\n",
        "print('Proportion of 0 m3 Consumption value by Consumer_type [%]: \\n', df[df.Consumption == 0].groupby('Consumer_type')['Consumer_number'].count()/df.groupby('Consumer_type')['Consumer_number'].count()*100, sep='', end='\\n\\n')\n",
        "\n",
        "# Barplot of the above data as percentages\n",
        "fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
        "(df[df.Consumption == 0].groupby('Year')['Consumer_number'].count()/len(df[df.Consumption == 0])*100).plot(kind='bar', ax=ax[0])\n",
        "(df[df.Consumption == 0].groupby('Month')['Consumer_number'].count()/len(df[df.Consumption == 0])*100).plot(kind='bar', ax=ax[1])\n",
        "(df[df.Consumption == 0].groupby('Consumer_type')['Consumer_number'].count()/df.groupby('Consumer_type')['Consumer_number'].count()*100).plot(kind='bar', ax=ax[2])\n",
        "ax[0].set_title('Proportion by Year')\n",
        "ax[1].set_title('Proportion by Month')\n",
        "ax[2].set_title('Proportion by Consumer_type')\n",
        "ax[0].set_ylabel('Percentage')\n",
        "ax[1].set_ylabel('Percentage')\n",
        "ax[2].set_ylabel('Percentage')\n",
        "ax[2].set_xticklabels(ax[2].get_xticklabels(), rotation=45)\n",
        "fig.suptitle('Proportion of 0 m3 Consumption value by Year, Month and Consumer_type', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43131dd4",
      "metadata": {},
      "source": [
        "#### Percentage of 0 m3 Consumption value for each month and Consumer type\n",
        "\n",
        "In order to see if there is a different monthly behaviour for each Consumer type, we can plot the percentage of zero values for each month and Consumer type. First, compute the number of zero values by consumer type and month, and then normalize it by each consumer type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff19d0fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "percent_type_month = df[df.Consumption == 0].groupby(['Consumer_type','Month'])['Consumer_number'].count()/df[df.Consumption == 0].groupby('Consumer_type')['Consumer_number'].count()*100\n",
        "df_type_month = percent_type_month.unstack().T\n",
        "display(df_type_month)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de21741",
      "metadata": {},
      "source": [
        "(*In order to see the Plotly interactive plots, download and run the notebook in your local machine or in Google Colab. Attached screenshot to visualise the graph without running the notebook.*)\n",
        "\n",
        "![Percentage of 0 m3 Consumption value for each month and Consumer type](zero-consumption-month-consumer-type.png)\n",
        "\n",
        "- Interactively filtering the graph traces by consumer, it is clearly visible that the three rural categories have a similar behaviour, with a peak in the months of January and December, and a minimum in the months of June, July and August. Also, the `construction` category shares a comparable trend with them, but with a higher peak in June.\n",
        "\n",
        "- The `domestic` and `industrial` show a similar and more stable behaviour through the year. `rural domestic` and `rural comercial` also have a similar trend to them until August, when they diverge.\n",
        "\n",
        "- The `low income families` category has a different behaviour from the rest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142b9d7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "total_counts = df.groupby(['Consumer_type'])['Consumer_number'].count()\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for consumer_type in df['Consumer_type'].unique():\n",
        "    \n",
        "    filtered = df_type_month[consumer_type]\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        mode='lines+markers',\n",
        "        x=filtered.index,\n",
        "        y=filtered.values,\n",
        "        name=f'{consumer_type}',\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "\n",
        "    title='Proportion of 0 m3 Consumption value by Month and Consumer Type',\n",
        "    xaxis_title='Month',\n",
        "    yaxis_title='Percentage of 0 m3 Consumption',\n",
        "    xaxis={'ticktext': ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], \n",
        "           'tickvals': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d596d9",
      "metadata": {},
      "source": [
        "### Creating a complete dataframe with all the possible combinations of year, month and consumer_number\n",
        "\n",
        "When we have missing data for a particular month and consumer, we actually doesn't have the entire sample (row), so we can't just replace the NaN (because they doesn't exist). So the idea is to create a complete dataframe with all the possible combinations of year, month and consumer_number, and then merge it with the original dataframe. This way, we will have a complete dataframe with all the months for each consumer_number, and the missing values will be NaN. Then we can do whatever we want with them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3807da83",
      "metadata": {},
      "source": [
        "In the following table we can see that for each consumer there are several months missing. For example, the consumer ``AABK96307399687530``, has missing values for the months 4,5,6,7. It is important to note that in this table when we have a NaN value, it means that we don't have any data for that particular month (throughout the years)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae089c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.groupby(['Consumer_number','Month'])['Consumption'].mean().unstack())\n",
        "# print(df.groupby(['Consumer_number','Month'])['Consumption'].mean().unstack().isna().sum(axis=0).value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c22d813",
      "metadata": {},
      "source": [
        "Average consumption values for each consumer type and month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3e37e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_mean = df.groupby(['Consumer_type','Month'])['Consumption'].mean().unstack()\n",
        "display(df_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db3de8f6",
      "metadata": {},
      "source": [
        "Itertools.product() is used to create all possible combinations of the 3 variables (year, month and Consumer type). Now, for each consumer, we have the rows for all the months, although some of them will be NaN. Then we have to decide what to do with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8059bfbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a complete DataFrame with all possible combinations of Year, Month, Consumer_type, and Consumer_number\n",
        "years = df['Year'].unique()\n",
        "months = df['Month'].unique()\n",
        "consumers = df['Consumer_number'].unique()\n",
        "\n",
        "# Product of all combinations\n",
        "all_combinations = product(years, months, consumers)\n",
        "# Convert to DataFrame\n",
        "complete_data = pd.DataFrame(all_combinations, columns=['Year', 'Month', 'Consumer_number'])\n",
        "\n",
        "#-------------------- Apply the same process to the competition dataset ---------------------\n",
        "years = comp_df['Year'].unique()\n",
        "months = comp_df['Month'].unique()\n",
        "consumers = comp_df['Consumer_number'].unique()\n",
        "\n",
        "# Product of all combinations\n",
        "all_combinations_comp = product(years, months, consumers)\n",
        "# Convert to DataFrame\n",
        "complete_data_comp = pd.DataFrame(all_combinations_comp, columns=['Year', 'Month', 'Consumer_number'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42156b6",
      "metadata": {},
      "source": [
        "Now we merge the new bigger dataframe with the old one to conserve the the other features (Installation_zone and Consumer_type)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04400c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine the complete DataFrame with the original DataFrame\n",
        "combined_data = pd.merge(complete_data, df, \n",
        "                         on=['Year', 'Month', 'Consumer_number'], \n",
        "                         how='left')\n",
        "\n",
        "print('Shape of the complete DataFrame: ', combined_data.shape)\n",
        "display(combined_data.head())\n",
        "\n",
        "#----------------------- Apply the same process to the competition dataset ---------------------\n",
        "combined_data_comp = pd.merge(complete_data_comp, comp_df, \n",
        "                         on=['Year', 'Month', 'Consumer_number'], \n",
        "                         how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52822d31",
      "metadata": {},
      "source": [
        "We could filter by a specific Consumer and notice that now we have all of the months present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff1ccb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_data.loc[combined_data.Consumer_number == \"AABK96307399687530\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e85b5c",
      "metadata": {},
      "source": [
        "Now we have to complete for each user the columns ``Consumer_type`` and ``Installation_zone``, with the data that we have for other months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef6bad0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating a dictionary with the mapping values\n",
        "consumer_type_mapping = df.drop_duplicates('Consumer_number').set_index('Consumer_number')['Consumer_type'].to_dict()\n",
        "installation_zone_mapping = df.drop_duplicates('Consumer_number').set_index('Consumer_number')['Installation_zone'].to_dict()\n",
        "\n",
        "# mapping the values\n",
        "combined_data['Consumer_type'] = combined_data['Consumer_number'].map(consumer_type_mapping)\n",
        "combined_data['Installation_zone'] = combined_data['Consumer_number'].map(installation_zone_mapping)\n",
        "\n",
        "combined_data.head()\n",
        "\n",
        "#----------------------- Apply the same process to the competition dataset ---------------------\n",
        "installation_zone_mapping_comp = comp_df.drop_duplicates('Consumer_number').set_index('Consumer_number')['Installation_zone'].to_dict()\n",
        "\n",
        "# mapping the values\n",
        "combined_data_comp['Installation_zone'] = combined_data_comp['Consumer_number'].map(installation_zone_mapping_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf2c3ec",
      "metadata": {},
      "source": [
        "Finally the dataset is complete, and now we have to deal with the NaN values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b9b877",
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_data.loc[combined_data.Consumer_number == \"AABK96307399687530\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6e453d",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = combined_data.copy()\n",
        "# Add Date column for easier manipulation\n",
        "df['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(Day=1))\n",
        "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "df.head()\n",
        "\n",
        "#----------------------- Apply the same process to the competition dataset ---------------------\n",
        "df_comp = combined_data_comp.copy()\n",
        "# Add Date column for easier manipulation\n",
        "df_comp['Date'] = pd.to_datetime(df_comp[['Year', 'Month']].assign(Day=1))\n",
        "df_comp['Date'] = df_comp['Date'].dt.strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76d55366",
      "metadata": {},
      "source": [
        "### Stats by Installation zone\n",
        "\n",
        "There are 49 Installation zones, but some of the Consumer types categories are concentrated in a few of them. In fact, there are zones where there is only one type of consumer (see heatmap, the numbers represent percentages for each consumer type). This feature could be useful for the classification task.\n",
        "\n",
        "- Broadly, the ``construction``, ``domestic``, ``industrial`` and ``low income families`` are located in the Installation zones 1 to 4 (with different distribution over them).\n",
        "\n",
        "- The rural comercial, rural domestic and rural expansion are more evenly distributed over the Installation zones, (specially rural domestic and rural expansion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d412987a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each Consumer_type, the Installation_zone where the are the most Consumers\n",
        "print(df.groupby('Consumer_type')['Installation_zone'].value_counts().groupby(level=0).head(), end='\\n\\n')\n",
        "\n",
        "# For each installation zone, the Consumer_type with the highest number of Consumers\n",
        "print(df.groupby('Installation_zone')['Consumer_type'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8407a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "cross = df.groupby(['Installation_zone', 'Consumer_type'])['Consumer_number'].count().unstack()\n",
        "cross.replace(np.nan, 0, inplace=True)\n",
        "\n",
        "# Normalize for each Consumer_type\n",
        "freqs = cross/cross.sum(axis=0)*100\n",
        "freqs = freqs.round(2)\n",
        "\n",
        "# Reorder the index Installation_zone by ascending number of zone, eg. Installation_zone 1, Installation_zone 2, etc.\n",
        "freqs = freqs.reindex(sorted(freqs.index, key=lambda x: int(x.split()[1])))\n",
        "print(freqs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e406c173",
      "metadata": {},
      "outputs": [],
      "source": [
        "freqs.sum(axis=1).plot(kind='bar', figsize=(15,5))\n",
        "plt.title('Number of Consumers per Installation_zone')\n",
        "plt.ylabel('Number of Consumers')\n",
        "print('Number of Consumers per Installation_zone: \\n', freqs.sum(axis=1), sep='', end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc80a7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(freqs, annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "plt.title(\"Heatmap of Consumer Types by Installation Zone\")\n",
        "# Rotate tick marks for visibility\n",
        "plt.xticks(rotation=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f578b10",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.loc[df['Consumption']!=0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431fe647",
      "metadata": {},
      "source": [
        "#### Histogram of Consumption values by Consumer type\n",
        "\n",
        "There are some outliers in the consumption values, specially for the ``industrial`` category. There are also some extreme values in the ``domestic``, ``construction`` and ``rural domestic`` categories. This could represent a measurement error or a real extreme value (eg. a big company with a high water consumption, refilling a pool, etc.). In any case, it is important to note that these outliers could affect the performance of the classification model. Also, the values are not normally distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30609c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histogram of Consumption values for each Consumer_type (excluding 0 values and before data cleaning)\n",
        "    \n",
        "fig = px.histogram(df.loc[df['Consumption']!=0], x='Consumption', nbins=3000, color='Consumer_type', barmode='overlay', histnorm='percent', title='Consumption distribution for each Consumer_type')\n",
        "fig.update_layout(\n",
        "    xaxis_title='Consumption [m3]',\n",
        "    yaxis_title='Percentage (by category) [%]',\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c43cd4",
      "metadata": {},
      "source": [
        "#### Boxplot of Consumption values by Consumer type\n",
        "\n",
        "Very high consumption values visible. Zoom in the graph to see the boxplots better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a9df01",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.box(df.loc[df['Consumption']!=0], y='Consumption', color='Consumer_type', title='Consumption distribution for each Consumer_type')\n",
        "fig.show()\n",
        "\n",
        "# Violin plot. Similar to boxplot but shows the distribution of the data\n",
        "# fig = px.violin(df.loc[(df['Consumption']!=0) & (df['Consumption']<600)], y='Consumption', color='Consumer_type', title='Consumption distribution for each Consumer_type')\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f668f69",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b29f43e7",
      "metadata": {},
      "source": [
        "### Data Cleaning\n",
        "\n",
        "It is necessary to clean the data in order to prepare it for the classification task.\n",
        "\n",
        "#### 1. Identify and process the missing values (remove or impute)\n",
        "\n",
        "We have to decide what to do with the NaN values. We could set a threshold and if there are more than X months missing for a particular consumer, we could remove it. If there is only missing data for a few months, we could impute them with the mean of the mean values for that month and consumer_type category that the user belongs to, and the year mean of the consumer, so that we get a balanced values and we don't change too much the distribution of the data.\n",
        "\n",
        "In this case, it is important to note the existence of the consumption value `0` in the feature `Consumption`. This could represent a missing value or actually, a zero water usage for that particular consumer in a specific month.\n",
        "\n",
        "#### 2. Remove the rows with negative consumption values\n",
        "\n",
        "There are no negative values in the dataset.\n",
        "\n",
        "#### 3. Remove the outliers\n",
        "\n",
        "There are some outliers in the consumption values, specially for the ``industrial`` category. There are also some extreme values in the ``domestic``, ``construction`` and ``rural domestic`` categories. This could represent a measurement error or a real extreme value (eg. a big company with a high water consumption, refilling a pool, etc.). In any case, it is important to note that these outliers could affect the performance of the classification model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f29c92",
      "metadata": {},
      "source": [
        "#### 1. NaN processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a37fbd",
      "metadata": {},
      "source": [
        "##### 1.1 Removing Data\n",
        "\n",
        "Remove the data for an entire year of a consumer if there are more than `n` months missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71873bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of months missing for each year and consumer\n",
        "\n",
        "nan_df = df.groupby(['Consumer_number','Year'])['Consumption'].apply(lambda x: x.isna().sum())\n",
        "display(nan_df.unstack())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ba0c21",
      "metadata": {},
      "source": [
        "Consumers that exceed that threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7f154c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consumers that exceed that threshold\n",
        "\n",
        "max_missing_months = 10\n",
        "nan_counts = nan_df.reset_index()\n",
        "to_remove = nan_counts[nan_counts['Consumption'] > max_missing_months]\n",
        "\n",
        "display(to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create and apply a mask to remove the consumers that exceed the threshold, now the size of the dataset has decreased to nearly 1000000 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "998a13bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove rows with more than 10 missing values\n",
        "mask = df.set_index(['Consumer_number', 'Year']).index.isin(to_remove.set_index(['Consumer_number', 'Year']).index)\n",
        "filtered_data = df[~mask]\n",
        "\n",
        "print(\"Size of the filtered_dataset: \", filtered_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can see that as we expect, there are NaN values for the months were the threshold is exceeded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521ba4a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_data.groupby(['Consumer_number','Year'])['Consumption'].apply(lambda x: x.isna().sum()).unstack()\n",
        "df = filtered_data.copy()\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7e75d70",
      "metadata": {},
      "source": [
        "#### 3. Outliers removal\n",
        "\n",
        "There are different available methods to remove outliers. Some of them are more robust than others or assume a normal distribution of the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea03221",
      "metadata": {},
      "source": [
        "##### 3.1 Interquartile Range method. IQR\n",
        "In this case, we can use the IQR method, which is an easy and robust method to remove outliers. The IQR is the difference between the 75th and 25th percentiles of the data. The outliers are defined as the values that are below the $\\text{25th percentile} - k \\cdot \\text{IQR}$ or above the $\\text{75th percentile} + k \\cdot \\text{IQR}$. The value of k is usually 1.5, but it can be adjusted to 3 depending on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64b3e89",
      "metadata": {},
      "outputs": [],
      "source": [
        "p25 = df.loc[df['Consumption']!=0].groupby('Consumer_type')['Consumption'].quantile(0.25)\n",
        "p75 = df.loc[df['Consumption']!=0].groupby('Consumer_type')['Consumption'].quantile(0.75)\n",
        "iqr = p75 - p25\n",
        "print('IQR: \\n', iqr, sep='', end='\\n\\n')\n",
        "k = 3\n",
        "upper = p75 + k*iqr\n",
        "# In this case, we are only interested in the upper limit for the outlier detection.\n",
        "print('Upper limit: \\n', upper, sep='', end='\\n\\n')\n",
        "print('Number of outliers for each Consumer_type: \\n', df.loc[df['Consumption']!=0].groupby('Consumer_type')['Consumption'].apply(lambda x: (x > upper[x.name]).sum()), sep='', end='\\n\\n')\n",
        "print('Percentage of outliers for each Consumer_type: \\n', df.loc[df['Consumption']!=0].groupby('Consumer_type')['Consumption'].apply(lambda x: (x > upper[x.name]).sum()/len(x)*100), sep='', end='\\n\\n')\n",
        "\n",
        "\n",
        "df['OutlierIQR'] = df.apply(lambda x: 1 if x['Consumption'] > upper[x['Consumer_type']] else 0, axis=1)\n",
        "print('Number of outliers: ', df.OutlierIQR.sum())\n",
        "\n",
        "df_clean = df.loc[df['OutlierIQR'] == 0].copy()\n",
        "df_clean.drop('OutlierIQR', axis=1, inplace=True)\n",
        "df_clean.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print('Shape of the original DataFrame: ', df.shape)\n",
        "print('Shape of the cleaned DataFrame: ', df_clean.shape)\n",
        "\n",
        "# #--------------------------------- Apply the same process to the competition dataset ---------------------\n",
        "# df_comp['OutlierIQR'] = df_comp.apply(lambda x: 1 if x['Consumption'] > upper[x['Consumer_type']] else 0, axis=1)\n",
        "# df_clean_comp = df_comp.loc[df_comp['OutlierIQR'] == 0].copy()\n",
        "# df_clean_comp.drop('OutlierIQR', axis=1, inplace=True)\n",
        "# df_clean_comp.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aa48453",
      "metadata": {},
      "source": [
        "##### 1.2 Imputing Data\n",
        "\n",
        "For the users with a number of missing months below the threshold, we can impute them with the average value of:\n",
        "\n",
        "- The mean consumption value of the consumer for a specific year\n",
        "- The mean consumption value of the ``Consumer_type`` category wich the consumer belongs to (for a specific year)\n",
        "- The monthly mean consumption value of the ``Consumer_type`` category wich the consumer belongs to (all the years)\n",
        "\n",
        "**Note: As we are going to compute the mean values for each consumer and year, it is better to remove the outliers first.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e335e85",
      "metadata": {},
      "source": [
        "First we compute these means, and then we merge them with the original dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f52918",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the mean of the Consumption values for each Consumer_number and Year\n",
        "consumer_yearly_avg = df_clean.groupby(['Consumer_number', 'Year'])['Consumption'].mean().reset_index(name='Consumer_Avg')\n",
        "\n",
        "# Compute the mean of the Consumption values for each Consumer_type and Year\n",
        "group_yearly_avg = df_clean.groupby(['Consumer_type', 'Year'])['Consumption'].mean().reset_index(name='Group_Avg')\n",
        "\n",
        "# Compute the monthly mean of the Consumption values for each Consumer_number (all years)\n",
        "group_monthly_avg = df_clean.groupby(['Consumer_type', 'Month'])['Consumption'].mean().reset_index(name='Group_Monthly_Avg')\n",
        "\n",
        "# Merge con el DataFrame original\n",
        "combined_data = pd.merge(df_clean, consumer_yearly_avg, on=['Consumer_number', 'Year'])\n",
        "combined_data = pd.merge(combined_data, group_yearly_avg, on=['Consumer_type', 'Year'])\n",
        "combined_data = pd.merge(combined_data, group_monthly_avg, on=['Consumer_type', 'Month'])\n",
        "\n",
        "display(combined_data.head)\n",
        "\n",
        "#--------------------------------- Apply the same process to the competition dataset ---------------------\n",
        "# Compute the mean of the Consumption values for each Consumer_number and Year\n",
        "consumer_yearly_avg_comp = df_comp.groupby(['Consumer_number', 'Year'])['Consumption'].mean().reset_index(name='Consumer_Avg')\n",
        "\n",
        "# We don't have the Consumer_type in the competition dataset, so we can't compute the group averages for the Consumer_type\n",
        "# # Compute the mean of the Consumption values for each Consumer_type and Year\n",
        "# group_yearly_avg_comp = df_clean_comp.groupby(['Consumer_type', 'Year'])['Consumption'].mean().reset_index(name='Group_Avg')\n",
        "\n",
        "# # Compute the monthly mean of the Consumption values for each Consumer_number (all years)\n",
        "# group_monthly_avg_comp = df_clean_comp.groupby(['Consumer_type', 'Month'])['Consumption'].mean().reset_index(name='Group_Monthly_Avg')\n",
        "\n",
        "# Merge con el DataFrame original\n",
        "combined_data_comp = pd.merge(df_comp, consumer_yearly_avg_comp, on=['Consumer_number', 'Year'])\n",
        "# combined_data_comp = pd.merge(combined_data_comp, group_yearly_avg_comp, on=['Consumer_type', 'Year'])\n",
        "# combined_data_comp = pd.merge(combined_data_comp, group_monthly_avg_comp, on=['Consumer_type', 'Month'])\n",
        "\n",
        "display(combined_data_comp.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5bb8b8d",
      "metadata": {},
      "source": [
        "Iterate over the rows of the dataframe and impute the NaN values with the corresponding mean values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09c1cc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Substitution of NaN values with the mean of the group, month and consumer\n",
        "combined_data['Consumption'] = combined_data.apply(\n",
        "    lambda row: ((row['Consumer_Avg'] + row['Group_Avg'] + row['Group_Monthly_Avg']) / 3 \n",
        "                 if pd.isna(row['Consumption']) else row['Consumption']), axis=1)\n",
        "\n",
        "\n",
        "#--------------------------------- Apply the same process to the competition dataset ---------------------\n",
        "combined_data_comp['Consumption'] = combined_data_comp.apply(\n",
        "    lambda row: ((row['Consumer_Avg'])  \n",
        "                 if pd.isna(row['Consumption']) else row['Consumption']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4a7189",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(combined_data.loc[combined_data.Consumer_number == \"AABK96307399687530\"].sort_values(by=['Year', 'Month']))\n",
        "df = combined_data.copy()\n",
        "\n",
        "#--------------------------------- Apply the same process to the competition dataset ---------------------\n",
        "df_comp = combined_data_comp.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd00e541",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)\n",
        "df.isna().any()\n",
        "\n",
        "df_comp.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead09a7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line graph of Compsumption vs Date for a specific Consumer_number \"AABK96307399687530\"\n",
        "fig = px.line(df.loc[df['Consumer_number'] == \"AABK96307399687530\"], x='Date', y='Consumption', title='Consumption vs Date for a specific Consumer_number')\n",
        "fig.update_layout(\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Consumption [m3]',\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430b0901",
      "metadata": {},
      "source": [
        "#### Graphs after removing outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb00a0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.histogram(df.loc[df['Consumption']!=0], x='Consumption', nbins=50, color='Consumer_type', barmode='overlay', histnorm='percent', title='Consumption distribution for each Consumer_type')\n",
        "fig.update_layout(\n",
        "    xaxis_title='Consumption [m3]',\n",
        "    yaxis_title='Percentage (by category) [%]',\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "\n",
        "fig = px.box(df.loc[(df['Consumption']!=0)], y='Consumption', color='Consumer_type', title='Consumption distribution for each Consumer_type')\n",
        "fig.show()\n",
        "\n",
        "fig = px.violin(df.loc[(df['Consumption']!=0)], y='Consumption', color='Consumer_type', title='Consumption distribution for each Consumer_type')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3be15cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the cleaned DataFrame as a parquet file\n",
        "df_clean = df.copy()\n",
        "df_clean.to_parquet('df_clean.parquet.gzip', compression='gzip')\n",
        "\n",
        "#--------------------------------- Apply the same process to the competition dataset ---------------------\n",
        "df_comp_clean = df_comp.copy()\n",
        "df_comp_clean.to_parquet('df_comp_clean.parquet.gzip', compression='gzip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4584dae",
      "metadata": {},
      "source": [
        "### Data Transformation\n",
        "\n",
        "#### 0. Train/test split\n",
        "\n",
        "First, we need to split the data into train and test sets. We will use the train set to train the model, and the test set to evaluate its performance. We will use a 80/20 split, which is a common practice. Also, this step should be done before any data transformation, in order to avoid data leakage, so the following steps parameters (encoding, scaling) should be trained only on the train set and then applied to the test set.\n",
        "\n",
        "It is important to note that the split should be stratified, in order to preserve the same proportion of samples for each class in both sets (``straify=y`` parameter in the ``train_test_split`` function). \n",
        "\n",
        "Note: *train/test split is the easiest way to evaluate the performance of a model, but it may not be the best one. In this case, we have a time series dataset, so it would be better to use a time-based split, in order to avoid data leakage. For example, we could use the first 6 years for training and the last year for testing. Also, it may be better not to separate the data from the same consumer in the train and test sets. We have to fix this.*\n",
        "\n",
        "#### 1. Feature encoding\n",
        "\n",
        "##### 1.1 Cyclical features\n",
        "\n",
        "The feature `Month` is cyclical. In order to take in account this dependance we can transform the features into two new ones, `sin` and `cos`, which will represent the circular nature of the data. This is called *cyclical encoding*.\n",
        "\n",
        "Some drawbacks to have in mind:\n",
        "- You are converting one information into two features.\n",
        "- Decision trees based algorithms (Random Forest, Gradient Boosted Trees, XGBoost) build their split rules according to one feature at a time. This means that they will fail to process these two features simultaneously whereas the cos/sin values are expected to be considered as one single coordinates system.\n",
        "\n",
        "##### 1.2 Categorical features\n",
        "\n",
        "The features `Consumer_type` and `Installation_zone` are categorical. In order to use them in the classification task, we need to encode them. There are different methods to do this:\n",
        "\n",
        "- One-hot encoding: creates a new column for each category, with a 1 if the sample belongs to that category, and 0 otherwise. This method is not recommended for high cardinality features (many categories), because it will create a lot of new features, which will increase the dimensionality of the dataset. In this case, we have 7 categories for `Consumer_type` and 49 for `Installation_zone`, so it is not recommended to use this method.\n",
        "- Label encoding: assigns a number to each category. One disadvantage of this method is that it will assign a numerical order to the categories, which could be interpreted as a ranking. (it is not recommended to use this method).\n",
        "- Frequency encoding/Target encoding: assigns a number to each category, based on the frequency of the category in the dataset. There are different methods to do this, like mean encoding, weight of evidence, etc.\n",
        "- Hashing encoding: creates a new column for each category, with a 1 if the sample belongs to that category, and 0 otherwise. It is similar to one-hot encoding, but it uses a hashing function to reduce the number of features. It is useful for high cardinality features (many categories), because it will create less new features than one-hot encoding. \n",
        "- Leave one out encoding: assigns a number to each category, based on the frequency of the category in the dataset, but leaving out the current sample. It is similar to target encoding, but it is more robust to overfitting, as it applies some kind of regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4278b6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0. Train/Test split\n",
        "\n",
        "# Helper function for splitting the data into train and test sets\n",
        "\n",
        "def split_by_id(df: pd.DataFrame, test_percent: float=0.2, random_state: int=123, plot: bool=True) -> (pd.DataFrame, pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Splits the dataframe into train and test, ensuring that no Consumer_number is present in both sets.\n",
        "\n",
        "    This function splits the dataset into training and testing, with different consumers in each set, while\n",
        "    maintaining the proportion of the target variable. It takes into account the Consumer_type variable, so that\n",
        "    the proportion of each Consumer_type is the same in both sets (aprox).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The dataset to be split.\n",
        "    test_percent : float, optional\n",
        "        The percentage of the dataset to be used for testing. The default is 0.2.\n",
        "    random_state : int, optional\n",
        "        The random state to be used for reproducibility. The default is 123.\n",
        "    plot : bool, optional\n",
        "        Whether to plot the distribution of the Consumer_type in the train and test sets. The default is True.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The training set.\n",
        "    pd.DataFrame\n",
        "        The testing set.\n",
        "    \"\"\"\n",
        "    id_by_type = df_clean.groupby('Consumer_type')['Consumer_number'].unique().to_dict()\n",
        "    df_id_by_type = pd.DataFrame(list(id_by_type.items()), columns=['Consumer_type', 'Consumer_number'])\n",
        "    df_id_by_type = df_id_by_type.explode('Consumer_number').reset_index(drop=True)\n",
        "    df_id_by_type.head()\n",
        "\n",
        "    # Split the Consumer_number into train and test while mantaining the proportion of the Consumer_type\n",
        "    train_consumers, test_consumers = train_test_split(df_id_by_type, test_size=test_percent, random_state=random_state, stratify=df_id_by_type['Consumer_type'])\n",
        "\n",
        "    train = df[df['Consumer_number'].isin(train_consumers['Consumer_number'])]\n",
        "    test = df[df['Consumer_number'].isin(test_consumers['Consumer_number'])]\n",
        "\n",
        "\n",
        "    if plot:\n",
        "        # Plot the distribution of the Consumer_type in the train and test sets\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Histogram(x=train.drop_duplicates(subset=['Consumer_number'])['Consumer_type'], name='Train'))\n",
        "        fig.add_trace(go.Histogram(x=test.drop_duplicates(subset=['Consumer_number'])['Consumer_type'],  name='Test'))\n",
        "        fig.update_layout(\n",
        "            title='Distribution of the Consumer_type in the train and test sets',\n",
        "            xaxis_title='Consumer_type',\n",
        "            yaxis_title='Number of Consumers',\n",
        "            barmode='overlay',\n",
        "        )\n",
        "        # Apply styling to the plot to be more professional, paper-like\n",
        "        # choose the figure font\n",
        "        font_dict=dict(\n",
        "                    size=12,\n",
        "                    color='black'\n",
        "                    )\n",
        "        # general figure formatting\n",
        "        fig.update_layout(font=font_dict,  # font formatting\n",
        "                        plot_bgcolor='white',  # background color\n",
        "                        width=850,  # figure width\n",
        "                        height=700,  # figure height\n",
        "                        margin=dict(r=10,t=50,b=10)  # remove white space \n",
        "                        )\n",
        "        # x and y-axis formatting\n",
        "        fig.update_yaxes(showline=True,  # add line at x=0\n",
        "                        linecolor='black',  # line color\n",
        "                        linewidth=2.4, # line size\n",
        "                        ticks='outside',  # ticks outside axis\n",
        "                        tickfont=font_dict, # tick label font\n",
        "                        mirror='allticks',  # add ticks to top/right axes\n",
        "                        tickwidth=2.4,  # tick width\n",
        "                        tickcolor='black',  # tick color\n",
        "                        )\n",
        "        fig.update_xaxes(showline=True,\n",
        "                        showticklabels=True,\n",
        "                        linecolor='black',\n",
        "                        linewidth=2.4,\n",
        "                        ticks='outside',\n",
        "                        tickfont=font_dict,\n",
        "                        mirror='allticks',\n",
        "                        tickwidth=2.4,\n",
        "                        tickcolor='black',\n",
        "                        )\n",
        "        fig.show()\n",
        "\n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71d79fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore de dataset from the parquet file\n",
        "# df_clean = pd.read_parquet('df_clean.parquet.gzip')\n",
        "\n",
        "train, test = split_by_id(df_clean, test_percent=0.2, random_state=123, plot=True)\n",
        "\n",
        "target = 'Consumer_type'\n",
        "features = ['Year', 'Month', 'Date', 'Installation_zone', 'Consumer_number', 'Consumption']\n",
        "\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_test = test[features]\n",
        "y_test = test[target]\n",
        "\n",
        "print('Train set shape: ', X_train.shape)\n",
        "print('Test set shape: ', X_test.shape)\n",
        "\n",
        "print('Train set target distribution: \\n', y_train.value_counts(normalize=True), sep='', end='\\n\\n')\n",
        "print('Test set target distribution: \\n', y_test.value_counts(normalize=True), sep='', end='\\n\\n')\n",
        "\n",
        "# 1. Baseline model\n",
        "\n",
        "# Baseline model - always predict the most frequent class\n",
        "y_pred = ['domestic']*len(y_test)\n",
        "print('------ Baseline Model ------')\n",
        "print('Accuracy score: ', accuracy_score(y_test, y_pred))\n",
        "print('Precision score: ', precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
        "print('Recall score: ', recall_score(y_test, y_pred, average='macro'))\n",
        "print('F1 score: ', f1_score(y_test, y_pred, average='macro'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "883e8efd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Encoding the cyclical feature \"Month\"\n",
        "\n",
        "# (df.Month-1) because we want to start from 0, not 1\n",
        "X_train['Month_sin'] = np.sin((X_train.Month-1)*(2.*np.pi/12))\n",
        "X_train['Month_cos'] = np.cos((X_train.Month-1)*(2.*np.pi/12))\n",
        "\n",
        "X_test['Month_sin'] = np.sin((X_test.Month-1)*(2.*np.pi/12))\n",
        "X_test['Month_cos'] = np.cos((X_test.Month-1)*(2.*np.pi/12))\n",
        "\n",
        "# Parametric plot of the cyclical feature \"Month\"\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X_train.Month_sin, X_train.Month_cos)\n",
        "plt.xlabel('Month_sin')\n",
        "plt.ylabel('Month_cos')\n",
        "plt.title('Parametric plot of the cyclical feature \"Month\"')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f46f7d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2 Encoding the rest of the categorical features\n",
        "\n",
        "# Encoding of the target variable. It will assign a number to each category.\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "print(le.classes_)\n",
        "print(le.transform(le.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd3f2d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encoding of the categorical features\n",
        "\n",
        "# encoder = ce.LeaveOneOutEncoder(cols=['Installation_zone', 'Consumer_number'])\n",
        "# encoder = ce.CatBoostEncoder(cols=['Installation_zone', 'Consumer_number'])\n",
        "encoder = ce.TargetEncoder(cols=['Installation_zone'])\n",
        "encoder.fit(X_train, y_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)\n",
        "\n",
        "print(X_train.head(), '\\n')\n",
        "print(X_test.head())\n",
        "\n",
        "print(\"Installation zone nunique values: \", X_train.Installation_zone.nunique())\n",
        "print(\"Installation zone unique values: \", X_train.Installation_zone.unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c88554",
      "metadata": {},
      "source": [
        "#### 2. Feature scaling\n",
        "\n",
        "Necessary for some algorithms that are sensitive to the scale of the features or distance based, like SVM, KNN, Neural Netwworks, etc.\n",
        "\n",
        "##### 2.1 Standardization\n",
        "\n",
        "Standardization is a common method to scale the features. It transforms the data to have a mean of 0 and a standard deviation of 1. It is useful for features that follow a normal distribution. Afected by outliers. Computed as:\n",
        "\n",
        "$x_{scaled} = \\frac{x - \\mu}{\\sigma}$\n",
        "\n",
        "##### 2.2 Min-max scaling (Normalization)\n",
        "\n",
        "Min-max scaling transforms the data values to the range 0 - 1. Afected by outliers. It is calculated as: \n",
        "\n",
        "$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
        "\n",
        "\n",
        "##### 2.3 Robust scaling\n",
        "\n",
        "Similar to Standardization but instead of divide by $\\sigma$ (standard deviation), it uses the interquartile range (IQR) to scale the data. It is useful for features that have outliers, because it is more robust to them.\n",
        "\n",
        "$x_{scaled} = \\frac{x - \\mu}{Q_3 - Q_1}$\n",
        "\n",
        "##### 2.4 Power transformation\n",
        "\n",
        "Apply a power transformation to the data, in order to make it more Gaussian-like. Then standardize it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7289e06",
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = RobustScaler()\n",
        "# scaler = MinMaxScaler()\n",
        "X_train['Consumption'] = scaler.fit_transform(np.array(X_train['Consumption']).reshape(-1, 1))\n",
        "X_test['Consumption'] = scaler.transform(np.array(X_test['Consumption']).reshape(-1, 1))\n",
        "\n",
        "print(X_train[:5], '\\n')\n",
        "print(X_test[:5])\n",
        "print(X_train.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58531895",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series graph of consumption by consumer_type (scaled and without outliers)\n",
        "\n",
        "data = pd.concat([X_train, X_test], axis=0)\n",
        "data['Consumer_type'] = le.inverse_transform(pd.concat([pd.Series(y_train), pd.Series(y_test)], axis=0))\n",
        "\n",
        "group = data.groupby(['Date','Consumer_type'])['Consumption'].mean().unstack()\n",
        "\n",
        "fig = px.line(group, x=group.index, y=group.columns, title='Time series graph of consumption by consumer_type, scaled and without outliers')\n",
        "fig.update_layout(\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Consumption [m3]',\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0710711f",
      "metadata": {},
      "source": [
        "### Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c2eaa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "# 0. Train/Test split\n",
        "# Splitting the data into train and test sets\n",
        "target = 'Consumer_type'\n",
        "features = ['Year', 'Month', 'Date', 'Installation_zone', 'Consumption']\n",
        "\n",
        "# Sort dataframe by year\n",
        "df_clean = df.sort_values('Year')\n",
        "\n",
        "# Get the unique years and determine the split\n",
        "unique_years = df_clean['Year'].unique()\n",
        "train_years = unique_years[:3]\n",
        "test_years = unique_years[-3:]\n",
        "\n",
        "# Create train and test dataframes\n",
        "train_df = df_clean[df_clean['Year'].isin(train_years)]\n",
        "test_df = df_clean[df_clean['Year'].isin(test_years)]\n",
        "\n",
        "X_train = train_df[features].copy()\n",
        "y_train = train_df[target].copy()\n",
        "X_test = test_df[features].copy()\n",
        "y_test = test_df[target].copy()\n",
        "\n",
        "print('Train set shape: ', X_train.shape)\n",
        "print('Test set shape: ', X_test.shape)\n",
        "\n",
        "print('Train set target distribution: \\n', y_train.value_counts(normalize=True))\n",
        "print('Test set target distribution: \\n', y_test.value_counts(normalize=True))\n",
        "\n",
        "# 1.1 Encoding the cyclical feature \"Month\"\n",
        "# (df.Month-1) because we want to start from 0, not 1\n",
        "X_train['Month_sin'] = np.sin((X_train.Month-1)*(2.*np.pi/12))\n",
        "X_train['Month_cos'] = np.cos((X_train.Month-1)*(2.*np.pi/12))\n",
        "\n",
        "X_test['Month_sin'] = np.sin((X_test.Month-1)*(2.*np.pi/12))\n",
        "X_test['Month_cos'] = np.cos((X_test.Month-1)*(2.*np.pi/12))\n",
        "\n",
        "# 1.2 Encoding the rest of the categorical features\n",
        "# Encoding of the target variable. It will assign a number to each category.\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "print(le.classes_)\n",
        "print(le.transform(le.classes_))\n",
        "\n",
        "# Encoding of the categorical features\n",
        "encoder = ce.TargetEncoder(cols=['Installation_zone', 'Date'])\n",
        "encoder.fit(X_train, y_train)\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)\n",
        "\n",
        "print(X_train.head())\n",
        "print(X_test.head())\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train['Consumption'] = scaler.fit_transform(np.array(X_train['Consumption']).reshape(-1, 1))\n",
        "X_test['Consumption'] = scaler.transform(np.array(X_test['Consumption']).reshape(-1, 1))\n",
        "\n",
        "print(X_train[:5])\n",
        "print(X_test[:5])\n",
        "print(X_train.describe())\n",
        "\n",
        "# Apply SMOTE to balance the classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print('Resampled train set shape: ', X_train_res.shape)\n",
        "print('Resampled train set target distribution: \\n', pd.Series(y_train_res).value_counts(normalize=True))\n",
        "\n",
        "# 1. Baseline model\n",
        "# Baseline model - always predict the most frequent class\n",
        "y_pred = [np.argmax(np.bincount(y_train))]*len(y_test)\n",
        "print('------ Baseline Model ------')\n",
        "print('Accuracy score: ', accuracy_score(y_test, y_pred))\n",
        "print('Precision score: ', precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
        "print('Recall score: ', recall_score(y_test, y_pred, average='macro'))\n",
        "print('F1 score: ', f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e1ebf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RFC model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# train model\n",
        "model_rfc = RandomForestClassifier(n_estimators=100)\n",
        "model_rfc.fit(X_train, y_train)\n",
        "\n",
        "# make predictions within own data\n",
        "y_pred = model_rfc.predict(X_test)\n",
        "\n",
        "# evaluate model within own data\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f23532",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PREDICTION ON competition.csv\n",
        "\n",
        "df_competition = df_comp[features].copy()\n",
        "\n",
        "# 1.1 Encoding the cyclical feature \"Month\"\n",
        "# (df.Month-1) because we want to start from 0, not 1\n",
        "df_competition['Month_sin'] = np.sin((df_competition.Month-1)*(2.*np.pi/12))\n",
        "df_competition['Month_cos'] = np.cos((df_competition.Month-1)*(2.*np.pi/12))\n",
        "\n",
        "# Encoding of the categorical features\n",
        "df_competition = encoder.transform(df_competition)\n",
        "\n",
        "df_competition['Consumption'] = scaler.transform(np.array(df_competition['Consumption']).reshape(-1, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# use the trained models to make predictions on new data\n",
        "pred_rfc = model_rfc.predict(df_competition)\n",
        "\n",
        "# add the predictions to the competition DF\n",
        "df_rfc = df_competition.copy()\n",
        "df_rfc['Consumer_type'] = pred_rfc\n",
        "\n",
        "# save the prediction DF to a new .csv file\n",
        "df_rfc.to_csv('result_rfc.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad2013a",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df = df_rfc.copy()\n",
        "final_df['Consumer_number'] = df_comp['Consumer_number']\n",
        "final_df.groupby('Consumer_number')['Consumer_type'].apply(lambda x: statistics.mode(x))\n",
        "\n",
        "final_df['Consumer_type'] = le.inverse_transform(final_df['Consumer_type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d7de77",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from scipy import stats\n",
        "\n",
        "final_df = df_rfc.copy()\n",
        "final_df['Consumer_number'] = df_comp['Consumer_number']\n",
        "\n",
        "# Funcin para encontrar el valor menos repetido o la moda si todos son igualmente comunes\n",
        "def least_common_or_mode(serie):\n",
        "    counts = serie.value_counts()\n",
        "    # Si todos los valores son igualmente comunes, devuelve la moda\n",
        "    if counts.min() == counts.max():\n",
        "        try:\n",
        "            # Devuelve la moda si es nica\n",
        "            return stats.mode(serie)[0][0]\n",
        "        except:\n",
        "            # Si hay mltiples modas, puedes decidir qu hacer, por ejemplo, devolver el primer valor\n",
        "            return serie.iloc[0]\n",
        "    # De lo contrario, devuelve el valor menos comn\n",
        "    return counts.idxmin()\n",
        "\n",
        "# Aplicar la funcin para cada grupo\n",
        "final_df['Consumer_type'] = final_df.groupby('Consumer_number')['Consumer_type'].apply(least_common_or_mode)\n",
        "\n",
        "# Si necesitas invertir la transformacin de etiquetas\n",
        "# Asegrate de que no haya NaNs antes de invertir la transformacin de etiquetas\n",
        "final_df.Consumer_type.isna().sum()\n",
        "final_df['Consumer_type'] = le.inverse_transform(final_df['Consumer_type'].astype(int))\n",
        "\n",
        "final_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "418057f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df.Consumer_type.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u49f3kCIaW9D",
      "metadata": {
        "id": "u49f3kCIaW9D"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GPEgOUIdxQLf",
      "metadata": {
        "id": "GPEgOUIdxQLf"
      },
      "source": [
        "Old dataset analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "etj0YQsqgDlq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "etj0YQsqgDlq",
        "outputId": "4250ad49-508f-478e-8ea8-d2521dc1dce0"
      },
      "outputs": [],
      "source": [
        "def scatter_graph(consumertype_df, consumertype):\n",
        "  consumertype_df = df.loc[df['Consumer_type'] == consumertype]\n",
        "  consumer_month = consumertype_df['Month'].astype(str) + '/' + consumertype_df['Year'].astype(str)\n",
        "  consumer_water_consumed = [consumertype_df['Consumption']]\n",
        "\n",
        "  plt.figure(figsize=(15,4))\n",
        "  plt.scatter(consumer_month, consumer_water_consumed)\n",
        "\n",
        "  plt.title(\"scatter plot \" + consumertype)\n",
        "  plt.xlabel(\"month/year\")\n",
        "  plt.ylabel(\"water consumed\")\n",
        "  plt.xticks(rotation=70, size=8)\n",
        "  plt.show()\n",
        "\n",
        "scatter_graph(\"domestic_df\", \"domestic\")\n",
        "scatter_graph(\"rural_domestic_df\", \"rural domestic\")\n",
        "scatter_graph(\"industrial_df\", \"industrial\")\n",
        "scatter_graph(\"rural_commercial_df\", \"rural commercial\")\n",
        "scatter_graph(\"construction_df\", \"construction\")\n",
        "scatter_graph(\"low_income_families_df\", \"low income families\")\n",
        "scatter_graph(\"rural_expansion_df\", \"rural expansion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dcad7e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.loc[df['Consumer_type'] == consumertype]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8hcwhQMd1B-T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "8hcwhQMd1B-T",
        "outputId": "51742bd0-4d08-4431-9c83-e3fb42aceded"
      },
      "outputs": [],
      "source": [
        "xlabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dec']\n",
        "plt.figure(figsize=(15,4))\n",
        "\n",
        "def monthly_avarage_consumption(consumertype_df, consumertype):\n",
        "  consumertype_df = df.loc[df['Consumer_type'] == consumertype]\n",
        "  avg_consumption = consumertype_df[['Month', 'Consumption']].groupby('Month').mean()\n",
        "  plt.plot(xlabels, avg_consumption, label=consumertype)\n",
        "\n",
        "monthly_avarage_consumption(\"domestic_df\", \"domestic\")\n",
        "monthly_avarage_consumption(\"rural_domestic_df\", \"rural domestic\")\n",
        "monthly_avarage_consumption(\"industrial_df\", \"industrial\")\n",
        "monthly_avarage_consumption(\"rural_commercial_df\", \"rural commercial\")\n",
        "monthly_avarage_consumption(\"construction_df\", \"construction\")\n",
        "monthly_avarage_consumption(\"low_income_families_df\", \"low income families\")\n",
        "monthly_avarage_consumption(\"rural_expansion_df\", \"rural expansion\")\n",
        "\n",
        "plt.xlabel(\"month\", size=15)\n",
        "plt.ylabel(\"avarege water consumed\", size=15)\n",
        "plt.xticks(size=15)\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
        "          fancybox=True, shadow=True, ncol=5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mnrS-AkmlEDD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "mnrS-AkmlEDD",
        "outputId": "663bd1c5-0f30-41ca-a340-ef3ee2309dee"
      },
      "outputs": [],
      "source": [
        "xlabels = ('domestic', 'rural domestic', 'industrial', 'rural commercial', 'construction', 'low income families', 'rural expansion')\n",
        "avarage_consumption = df[['Consumption', 'Consumer_type']].groupby('Consumer_type').mean()\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.bar(xlabels, avarage_consumption.Consumption)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NhDopKJ04Bz1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "NhDopKJ04Bz1",
        "outputId": "27805da0-7f9f-4de7-e74c-f4a0cf6eeac1"
      },
      "outputs": [],
      "source": [
        "nozero_df = df.loc[df['Consumption'] != 0]\n",
        "xlabels = ('domestic', 'rural expansion', 'industrial', 'rural commercial', 'construction', 'low income families', 'rural expansion')\n",
        "\n",
        "domestic_list = nozero_df.loc[nozero_df['Consumer_type'] == 'domestic'] ['Consumption'].tolist()\n",
        "rural_domestic_list = nozero_df.loc[nozero_df['Consumer_type'] == 'rural domestic']['Consumption'].tolist()\n",
        "industrial_list = nozero_df.loc[nozero_df['Consumer_type'] == 'rural domestic']['Consumption'].tolist()\n",
        "rural_commercial_list = nozero_df.loc[nozero_df['Consumer_type'] == 'rural domestic']['Consumption'].tolist()\n",
        "construction_list = nozero_df.loc[nozero_df['Consumer_type'] == 'rural domestic']['Consumption'].tolist()\n",
        "low_income_families_list = nozero_df.loc[nozero_df['Consumer_type'] == 'rural domestic']['Consumption'].tolist()\n",
        "rural_expansion_list = nozero_df.loc[nozero_df['Consumer_type'] == 'rural domestic']['Consumption'].tolist()\n",
        "\n",
        "mode_consumption = [statistics.mode(domestic_list),\n",
        "                    statistics.mode(rural_domestic_list),\n",
        "                    statistics.mode(industrial_list),\n",
        "                    statistics.mode(rural_commercial_list),\n",
        "                    statistics.mode(construction_list),\n",
        "                    statistics.mode(low_income_families_list),\n",
        "                    statistics.mode(rural_expansion_list),\n",
        "                    ]\n",
        "\n",
        "median_consumption = [statistics.median(domestic_list),\n",
        "                      statistics.median(rural_domestic_list),\n",
        "                      statistics.median(industrial_list),\n",
        "                      statistics.median(rural_commercial_list),\n",
        "                      statistics.median(construction_list),\n",
        "                      statistics.median(low_income_families_list),\n",
        "                      statistics.median(rural_expansion_list),\n",
        "                    ]\n",
        "\n",
        "plt.bar(xlabels, mode_consumption)\n",
        "plt.show()\n",
        "\n",
        "plt.bar(xlabels, median_consumption)\n",
        "plt.show"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
